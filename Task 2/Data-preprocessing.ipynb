{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Review the dataset\n",
    "![Data](Data.PNG)\n",
    "\n",
    "The above datset shows Country, Age, Salary and Purchased. The field that we need to research is called `Dependenct field` and others are called `Independent Field`\n",
    "#### Dependent Field\n",
    "A dependent field is a whose outcome or value is derived from other field. In this example, we are trying to find Whether a customer made a purchase or not. Hence, `Purchased` here becomes the dependent field. In terms of coordinate, if we map this on graph then `Purchased` will be plotted on y-axis.\n",
    "\n",
    "#### Independent field\n",
    "Indendent fields are values that we observe. Example - If I stand on the side of freeway and start noting down the color of each car passed, whether it is a car or a truck, is it raining that day or sunny? etc. These values are called Independent fields. In the above datasheet, `Country`, `Age` and `Salary` are independent fields.\n",
    "\n",
    "In data science, everything is function. Hence if `Purchased` field = `Y` and `Country`, `Age` and `Salary` can be represented as `c`, `a` & `i` respectively. Then the above function can be represented as\n",
    "\n",
    " `Y = Xc + Ta + Zi` \n",
    " \n",
    " ### Importing the essential library\n",
    " There are three essential library for most basic machine learning projects. Add the below library\n",
    " \n",
    " `\n",
    " import numpy as np\n",
    " import matplotlib.pyplot as plt\n",
    " import pandas as pd\n",
    " `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the dataset\n",
    "Use pandas to import the file data.csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country</th>\n",
       "      <th>Age</th>\n",
       "      <th>Salary</th>\n",
       "      <th>Purchased</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>France</td>\n",
       "      <td>44.0</td>\n",
       "      <td>72000.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Spain</td>\n",
       "      <td>27.0</td>\n",
       "      <td>48000.0</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Germany</td>\n",
       "      <td>30.0</td>\n",
       "      <td>54000.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Spain</td>\n",
       "      <td>38.0</td>\n",
       "      <td>61000.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Germany</td>\n",
       "      <td>40.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>France</td>\n",
       "      <td>35.0</td>\n",
       "      <td>58000.0</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Spain</td>\n",
       "      <td>NaN</td>\n",
       "      <td>52000.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>France</td>\n",
       "      <td>48.0</td>\n",
       "      <td>79000.0</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Germany</td>\n",
       "      <td>50.0</td>\n",
       "      <td>83000.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>France</td>\n",
       "      <td>37.0</td>\n",
       "      <td>67000.0</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Country   Age   Salary Purchased\n",
       "0   France  44.0  72000.0        No\n",
       "1    Spain  27.0  48000.0       Yes\n",
       "2  Germany  30.0  54000.0        No\n",
       "3    Spain  38.0  61000.0        No\n",
       "4  Germany  40.0      NaN       Yes\n",
       "5   France  35.0  58000.0       Yes\n",
       "6    Spain   NaN  52000.0        No\n",
       "7   France  48.0  79000.0       Yes\n",
       "8  Germany  50.0  83000.0        No\n",
       "9   France  37.0  67000.0       Yes"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import the dataset\n",
    "dataset = pd.read_csv(\"data.csv\",na_values=[\"$$\"])\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the dependent and Indepdent Variable matrix of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = dataset.iloc[:, :3].values \n",
    "y = dataset.iloc[:, 3:4].values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify the missing data\n",
    "In real world when you are handed a dataset. You will find that lot of data might be missing. Ex - `Row 6 Salary` is empty and `Row 8 Age` is missing in our data.csv\n",
    "![Dataset](csvdata.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Startegies to handle missing data\n",
    "- Remove the rows that have missing data. This approach is simplest but will skew our dataset if lot of fields are missing.\n",
    "- Replace the missing data with Mean\n",
    "- Replace the missing data with Median\n",
    "- Replace the missing data with Mode or Frequency\n",
    "\n",
    "As a general rule of thumb `mean`,`median`,`most_frequent`,`constant`  and  can be applied to numerical values only. It cannot be applied to alphnumeric or String values. This is where we can use Mode or Frequency.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taking Care of missing data * (Numerical values)*\n",
    "`Scikit` library in python provides a class called `Imputer` which helps in fixing the missing values using the above strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(missing_values = np.nan, strategy='mean')\n",
    "imputer = imputer.fit(x[:,1:3])\n",
    "x[:, 1:3] = imputer.transform(x[:, 1:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code we define a `SimpleImputer` class object.\n",
    "`imputer = SimpleImputer(missing_values = np.nan, strategy='mean')`\n",
    "\n",
    "The next line `imputer = imputer.fit(x[:,1:3])` is telling imputer object on which matrix columns it needs to look for missing value and mark them for filling value as mean\n",
    "\n",
    "The last line `x[:, 1:3] = imputer.transform(x[:, 1:3])` fills in the cells of X martix that were marked by the imputer.\n",
    "\n",
    "*One thing that I wanted to try was that would happen if I just used `imputer.fit(x)` and did not provide the columns which actually had the missing values. What happens is that since you have marked the startegy as `strategy='mean'`, the imputer will try to take average/mean of each row `country`, `Age` & `Salary`. Since `Country` is of type String, python will complain about it. However if the `Country` column was numeric, then this `imputer.fit(x)` would have worked. Additionally, on the left side too we have defined that data be copied into x[:, 1:3]. This is because if you write `x = imputer.transform(x[:, 1:3])`, imputer has marked the cell to be transformed is `(1,5)` as there are two columns it is looking at `Age` and `Salary`  but if you put only x on the left side, then `(1,5)` refers to `Age` 40, while imputer is expecting it to be a `Salary` column.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['France', 44.0, 72000.0],\n",
       "       ['Spain', 27.0, 48000.0],\n",
       "       ['Germany', 30.0, 54000.0],\n",
       "       ['Spain', 38.0, 61000.0],\n",
       "       ['Germany', 40.0, 63777.77777777778],\n",
       "       ['France', 35.0, 58000.0],\n",
       "       ['Spain', 38.77777777777778, 52000.0],\n",
       "       ['France', 48.0, 79000.0],\n",
       "       ['Germany', 50.0, 83000.0],\n",
       "       ['France', 37.0, 67000.0]], dtype=object)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ImputedResult](imputedResult.PNG)\n",
    "\n",
    "As you can see, the imputer has filled in the missing values for `Salary` as $63777.78  and `Age` as 38.7.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taking care of Categorical data\n",
    "Categorical data is any data that is not numeric. Think of it as attributes of an object. Ex - If I am observing cars on a freeway and noting down the speed at which the car is driving and the color of the car. The cars may have different colors such as red, green, blue etc. I can only observe them but cannot do anything else with them like add, subtract, find mean or anything. However, a machine learning algorithm may see value in finding a relation between dependent and independent feature. The algorithm can only make use of it, if it is defined in numbers.\n",
    "\n",
    "One way to define them is by assigning them numbers ex- Red =1, Blue =2 and so on.\n",
    "In our current example we have a categorical data - `Country`. To convert or encode as it is called in ML, we will again use a library from `Scikit`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 44.0, 72000.0],\n",
       "       [2, 27.0, 48000.0],\n",
       "       [1, 30.0, 54000.0],\n",
       "       [2, 38.0, 61000.0],\n",
       "       [1, 40.0, 63777.77777777778],\n",
       "       [0, 35.0, 58000.0],\n",
       "       [2, 38.77777777777778, 52000.0],\n",
       "       [0, 48.0, 79000.0],\n",
       "       [1, 50.0, 83000.0],\n",
       "       [0, 37.0, 67000.0]], dtype=object)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "labelEncoder_x = LabelEncoder()\n",
    "x[: ,0] =labelEncoder_x.fit_transform(x[:, 0])\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code uses `LabelEncoder`  class to encode the values of country. Here, France =0, Spain = 2, Germany=1\n",
    "There is however a simple problem here.\n",
    "![encoded](encoded.PNG)\n",
    "#####Problem:\n",
    "The modesl are based on equation. Since the LabelEncoder here, has assigned them values 0,1,2, the equation would think that Germany has highre value than France and Spain has higher value than Germany. This certainly is not the case. These are supposed to be treated as observational values. Example - Picking on our earlier example of observing car speeds and color. If we use LabelEncoder for encoding car colors, the model may comeback and say that A `red Prius` will always be faster than a `White Ferari`\n",
    "\n",
    "To get over this problem and use category as markers, we will use another class which will create dummy encoding and this give equal value to all categorical data. It does that by creating a sparse matrix.\n",
    "![dummyEncoded](dummyEncoding.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "oneHotEncoder = OneHotEncoder(categories='auto')\n",
    "x= oneHotEncoder.fit_transform(x).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So after dummy encoding, the complete sparse matrix of `x` looks like this\n",
    "![dummyEncodingResult.PNG](dummyEncodingResult.PNG)\n",
    "\n",
    "So now let's convert the dependent coloumn `Purchased` as well. However, we do not need to use dummy encoding as there are only two varibales and that it is a dependent varibale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GRIET\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "labelEncoder_y = LabelEncoder()\n",
    "y =labelEncoder_y.fit_transform(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting dataset into training and test set\n",
    "- **Why do we need to split the data into training and test set?**\n",
    "  1. This is about algorithm that will create the equation to predict the result of new information based on history. If you let it run on all of the data then it will learn too much and will have correlation value or in other words overfitting.  A simple example in real world is of a boy who memorizes the book word by word but fails in the actual exam, because instead of asking what is 2 +2 like he read in the book, the exam asked what is 1+3. The student learnt too much but cannot relate or imply the same knowledge on a new data set.\n",
    "  \n",
    "  2. Sometimes you may have limited data to build the model and may not additional data to test your model.\n",
    "  \n",
    "- ** What is a good ratio to split the data?**\n",
    "    Usually 80/20 or upto 70/30 is a good number. Going beyond 70/30 is not recommended. \n",
    "\n",
    "To split a dataset into taining and test set. We will use another class called `train_test_split`, which returns 4 different values - Traiing_X, Test_X,Traiing_Y, Test_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is now spllit into 8 and 2 observation.\n",
    "\n",
    "![tarinTestResult](tarinTestResult.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling/ Normalization/Standardization\n",
    "If you look your dataset and pay attention to independent features `Age` and `Salary`, the range varies for `Age` between `27 - 50` and `Salary` between `48000 - 83000`. When an equation is created, the distance between two datapoints is huge and the values can be skewed because one of the columns have 27 for age and 83000 for salary. \n",
    "\n",
    "![ecd](ecd.PNG)\n",
    "The models are usualy based on Euclidean distance. In our case since Salary has a higher range, it will dominate the age values which means when we do distance between observation  (27, 48000) and (48, 79000) then (x2-x1) ^2  vs (y2-y1) ^2 is \n",
    "441 vs 961000000. Hence Age is oversahdowed by Salary.\n",
    "\n",
    "![zstd](zstd.PNG)\n",
    "\n",
    "If you are from statstics field, you probably are already familiar with standardization or Z-index etc. Above is the two way to calculate the standard range which will always give us a value between  **0 and 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc_x = StandardScaler()\n",
    "x_train = sc_x.fit_transform(x_train)\n",
    "x_test = sc_x.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE** : We are not fitting and transforming the `x_test` because it is already fitted based on `xtrain` so that they are now on same scale. If we would have used fit_transform on both test and train then their scale would have been different say one could between -1 and +1 while the other could be on -3 and +3\n",
    "![xtrainScaled.PNG](xtrainScaled.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **Why did we not apply scaling on `y` or dependent variable?**\n",
    "  In this case the values are 0 and 1 only or in other words this is a classification problem whcih has two choices, whether a customer bought the product or he did not buy the product. In other scenario such as that realted to multiple regression , we may have sitituation where we may have to do feature scaling on `y` as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constant Factor Normalization (or) Decimal Scaling :\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This technique scales the values of a feature by dividing the values of a feature by a power of 10.\n",
    "![images](images.png)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "347.85px",
    "left": "1541px",
    "right": "21px",
    "top": "120px",
    "width": "358px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
